# -*- coding: utf-8 -*-
"""tanzim_422_lab_Assign5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VAVZzyNeKAH3wOsbsQe_mWbqtwUCFbc_
"""

#importing necessary libraries
import pandas as pd
import numpy as np

df=pd.read_csv('mushroom edibility classification dataset.csv')
df.head()

df.shape

#Dropping the 'Unnamed:0' column
df.drop('Unnamed: 0', axis=1, inplace=True)

df.shape

df.head()

df.isnull().sum()

#dropping the rows with null values
print("Shape of dataframe before dropping:", df.shape)
df = df.dropna(axis = 0, subset = ['cap-shape','cap-color'])
print("Shape after dropping:", df.shape)

#check whether null values succesfully removed
df.isnull().sum()

#now going to encode categorical values
#first check which columns have categorical values
df.info()

#checking unique values
df['class'].unique()

df['bruises'].unique()

#encoding categorical values
from sklearn.preprocessing import LabelEncoder

# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "class" column
df['class'] = enc.fit_transform(df['class'])

print(df[['class']].head())

# Apply the encoding to the "bruises" column

df['bruises'] = enc.fit_transform(df['bruises'])

print(df[['bruises']].head(10))

df.head()

#feature scaling
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaler.fit(df)

# transform data
df_scaled = scaler.transform(df)

#Now applying logistic regression

# Import the dependencies for logistic regression
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

X = df_scaled[:,1:]
y = df_scaled[:,0]

#Split the data into 80% training and 20% testing
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Train the model
model = LogisticRegression()
model.fit(x_train, y_train) #Training the model
predictions = model.predict(x_test)
print(predictions)# printing predictions

score_lr = accuracy_score(y_test, predictions)
print(score_lr)

#Now doing the task using decision tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

X = df_scaled[:,1:]
y = df_scaled[:,0]

#Split the data into 80% training and 20% testing
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier(criterion='entropy',random_state=1)
clf.fit(x_train,y_train)
y_pred = clf.predict(x_test)
score_dt=accuracy_score(y_pred,y_test)
print(score_dt)

#Comparing the accuracy and plotting them as a bar chart using matplotlib
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt

objects = ('Logistic Regression', 'Decision Tree')
y_pos = np.arange(len(objects))
performance = [score_lr, score_dt]

plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('Accuracy')
plt.xlabel('Algorithm')
plt.title('Bar chart Comparing Accuracy')

plt.show()

